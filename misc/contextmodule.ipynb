{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Conv2d, Sequential, ReLU\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ContextModule(input_channels, kernel_size=[3, 3], img_size=512):\n",
    "    \"\"\"\n",
    "    Builds the context module block block for MobileNets\n",
    "    based on: \"MULTI-SCALE CONTEXT AGGREGATION BYDILATED CONVOLUTIONS\"\n",
    "    http://vladlen.info/papers/dilated-convolutions.pdf\n",
    "    Architecture:\n",
    "    Layer           1   2   3   4       5       6       7       8\n",
    "    Convolution     3×3 3×3 3×3 3×3     3×3     3×3     3×3     1×1\n",
    "    Dilation        1   1   2   4       8       16      1       1 \n",
    "    Truncation      Yes Yes Yes Yes     Yes     Yes     Yes     No\n",
    "    Receptive field 3×3 5×5 9×9 17×17   33×33   65×65   67×67   67×67\n",
    "    Convolution feature map size formula: \n",
    "    The context module is designed to increase the performance of dense prediction architectures by\n",
    "    aggregating multi-scale contextual information. The module takes C feature maps as input and\n",
    "    produces C feature maps as output. The input and output have the same form, thus the module can\n",
    "    be plugged into existing dense prediction architectures.\n",
    "    \"\"\"\n",
    "    net = []\n",
    "    dilations = [1,1,2,4,8,16]\n",
    "\n",
    "    for d in dilations:\n",
    "        # Get padding to keep output shape same as input shape\n",
    "        # o = [i + 2*p - k - (k-1)*(d-1)]/s + 1\n",
    "        k = kernel_size if isinstance(kernel_size, int) else kernel_size[0]\n",
    "        #pad = ((img_size - 1 ) / s - img_size + k + (k-1)*(d-1)) / 2  --- since s will be 1, simplify:\n",
    "        pad = int((k + (k-1)*(d-1) - 1) / 2)\n",
    "        print(\"padding: %d\"%pad)\n",
    "        net.append( Conv2d(in_channels = input_channels, \n",
    "                           out_channels = input_channels, \n",
    "                           kernel_size=kernel_size, \n",
    "                           padding=pad, \n",
    "                           dilation=d) )\n",
    "\n",
    "    # Pointwise\n",
    "    net.append( Conv2d(in_channels = input_channels, out_channels = input_channels, kernel_size=[1, 1]) )\n",
    "\n",
    "    #net.append( BatchNorm2d(input_channels) )\n",
    "\n",
    "    #net.append( ReLU() )\n",
    "\n",
    "    return Sequential(*net)\n",
    "    #####################\n",
    "    #      End ContextModule     \n",
    "    #####################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding: 1\n",
      "padding: 1\n",
      "padding: 2\n",
      "padding: 4\n",
      "padding: 8\n",
      "padding: 16\n"
     ]
    }
   ],
   "source": [
    "cmod = ContextModule(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.random.rand(1,1,512,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 1, 512, 512), 0.9999980216352864, 1.2225622584294271e-06)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape, img.max(), img.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgTensor = torch.FloatTensor(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = cmod(imgTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3652, -0.3625, -0.3657,  ..., -0.3331, -0.3319, -0.3357],\n",
       "          [-0.3656, -0.3619, -0.3658,  ..., -0.3319, -0.3341, -0.3319],\n",
       "          [-0.3652, -0.3641, -0.3642,  ..., -0.3348, -0.3312, -0.3334],\n",
       "          ...,\n",
       "          [-0.3252, -0.3141, -0.3234,  ..., -0.3556, -0.3515, -0.3524],\n",
       "          [-0.3163, -0.3205, -0.3190,  ..., -0.3517, -0.3521, -0.3539],\n",
       "          [-0.3224, -0.3158, -0.3214,  ..., -0.3527, -0.3541, -0.3501]]]],\n",
       "       grad_fn=<ThnnConv2DBackward>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.4839, 0.2239, 0.2471,  ..., 0.0078, 0.8321, 0.8030],\n",
       "          [0.8905, 0.6168, 0.6412,  ..., 0.6550, 0.6975, 0.6839],\n",
       "          [0.6794, 0.9819, 0.7558,  ..., 0.3268, 0.9318, 0.5718],\n",
       "          ...,\n",
       "          [0.5202, 0.6571, 0.9101,  ..., 0.1338, 0.1855, 0.0876],\n",
       "          [0.1861, 0.5415, 0.9671,  ..., 0.5816, 0.4401, 0.3209],\n",
       "          [0.6829, 0.9410, 0.0272,  ..., 0.8861, 0.6290, 0.0989]]]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgTensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512, 512])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
